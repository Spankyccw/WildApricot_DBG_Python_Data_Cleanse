# Title: DBG_GOT_2024_Data_Cleanse
# Author: cwilliams & MS Copilot
# Date: 2025/09/08
# Purpose: Clean Durango Botanic Gardens "Garden on Tour" 2024 data clean before loading with Wild Apricot CMS
# Remove columns that are not needed for the data import/merge of Contacts "GOT 2024" column.
# Add column to set Contacts "GOT 2024" column.
# Dependencies: pandas
# Date/Name/Change
# 09/08/2025 Initial version
# 09/10/2025 Trying to add dynamic output filename with datetime, when executed ask if input file should be the last output filename
# as we loop to refine while retaining all the files along the way. Edit python to refine files formatting etc.
# 09/11/2025 Modularizing and testing as functions are added to main. Need to version control this work in GitHub.
# 09/11/2025 Code improvements and debugging assistance provided by Claude (Anthropic AI) - enhanced error handling,
# data validation functions, phone number formatting for Wild Apricot compatibility, conditional output logic,
# function definition order fixes, interactive data cleansing workflow optimization, and comprehensive reporting
# with identifying fields for efficient source data correction.
# 09/11/2025 Claude petered out on me so using more piece meal AI assist with MSCopilot. Added some detail so see which phone numbers were modified.
# 09/13/2025 Experimented with using module usaddress for standardization of address information.
# 09/16/2025 Added comprehensive logging function to track all corrections with ID fields
# 09/16/2025 Added address standardization functions with street type and directional abbreviations with Anthropic's Claude AI tool

from datetime import datetime
import os
import sys
import pandas as pd
import re
import glob
import logging

# Use raw string (r'') to handle spaces in the file path
#input
file_path1 = r'C:\Users\Charl\OneDrive\Documents\Development\Python\DBG\GoT 2024 Attendance ccw.xlsx'
#output, temp name
file_path2 = r'C:\Users\Charl\OneDrive\Documents\Development\Python\DBG\GoT 2024 Attendance ccw clean.xlsx'

# Reset output filename to be dynamic.
# Original file name (without extension)
base_name2 = "GoT_2024_Attendance_ccw_clean"

# Get current datetime in YYYYMMDD_HHMM format
datetime_stamp = datetime.now().strftime('%Y%m%d_%H%M')

# Append datetime to base name
new_file_name2 = f"{base_name2}_{datetime_stamp}.xlsx"

# Optional: build full path
output_dir2 = r'C:\Users\Charl\OneDrive\Documents\Development\Python\DBG'
full_path2 = os.path.join(output_dir2, new_file_name2)

# Setup logging configuration
log_filename = f"DBG_data_cleanse_{datetime_stamp}.log"
log_filepath = os.path.join(output_dir2, log_filename)

def setup_logging(log_filepath):
    """
    Setup logging configuration for data cleaning operations.
    
    Parameters:
        log_filepath (str): Full path for the log file
    
    Returns:
        logging.Logger: Configured logger instance
    """
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filepath, mode='w'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    logger = logging.getLogger(__name__)
    return logger

def log_correction(logger, correction_type, row_data, old_value, new_value, field_name):
    """
    Log individual corrections with identifying information.
    
    Parameters:
        logger: Logger instance
        correction_type (str): Type of correction made
        row_data (pd.Series): Row data containing identifying fields
        old_value (str): Original value before correction
        new_value (str): New value after correction
        field_name (str): Name of the field being corrected
    """
    # Extract identifying fields safely
    first_name = row_data.get('First name', 'N/A')
    last_name = row_data.get('Last name', 'N/A')
    email = row_data.get('email', 'N/A')
    phone = row_data.get('Phone', 'N/A')
    
    logger.info(f"{correction_type} - {field_name}: '{old_value}' -> '{new_value}' | "
                f"Name: {first_name} {last_name} | Email: {email} | Phone: {phone}")

def clean_contact_fields_with_logging(df, logger):
    """
    Identifies and removes leading/trailing spaces from 'email' and 'Phone' columns.
    Logs individual corrections and provides summary.

    Parameters:
        df (pd.DataFrame): The DataFrame containing 'email' and 'Phone' columns.
        logger: Logger instance for recording corrections

    Returns:
        pd.DataFrame: A copy of the DataFrame with cleaned 'email' and 'Phone' fields.
    """
    df_cleaned = df.copy()
    logger.info("üîç Starting contact field cleaning (email and Phone columns)")
    
    total_changes = 0
    changes_by_column = {}

    for col in ['email', 'Phone']:
        if col in df_cleaned.columns:
            column_changes = 0
            # Convert to string for safe string operations
            df_cleaned[col] = df_cleaned[col].astype(str)

            # Identify rows with leading/trailing spaces
            spaces_mask = df_cleaned[col].str.startswith(' ', na=False) | df_cleaned[col].str.endswith(' ', na=False)
            
            # Log individual corrections
            for idx in df_cleaned[spaces_mask].index:
                row = df_cleaned.loc[idx]
                old_value = row[col]
                new_value = row[col].strip()
                
                log_correction(logger, "SPACE_CLEANUP", row, old_value, new_value, col)
                df_cleaned.loc[idx, col] = new_value
                column_changes += 1

            total_changes += column_changes
            changes_by_column[col] = column_changes

    # Summary logging
    logger.info(f"üìä Contact field cleaning summary:")
    for col, changes in changes_by_column.items():
        logger.info(f"   - {col}: {changes} corrections made")
    logger.info(f"üßΩ Contact field cleaning completed: {total_changes} total changes across {len(df_cleaned)} rows")
    
    return df_cleaned

# üßº Clean phone numbers: remove "1-" prefix, spaces, dashes, parentheses
def clean_phone_number(phone_value):
    if pd.isna(phone_value):
        return ''
    
    cleaned = str(phone_value).strip()
    
    if cleaned.startswith("1-"):
        cleaned = cleaned[2:]
    
    # Remove spaces, dashes, parentheses
    cleaned = re.sub(r'[\s\-\(\)]', '', cleaned)
    
    return cleaned

# üîß Format clean phone numbers into 999-999-9999 format
def format_phone_number(clean_phone):
    """
    Format a clean 10-digit phone number into 999-999-9999 format.
    
    Parameters:
        clean_phone (str): Clean phone number (digits only)
    
    Returns:
        str: Formatted phone number or original if invalid
    """
    if len(clean_phone) == 10 and clean_phone.isdigit():
        return f"{clean_phone[:3]}-{clean_phone[3:6]}-{clean_phone[6:]}"
    return clean_phone  # Return original if not valid 10 digits

def get_invalid_phone_number(df1, logger, first_name_col='First name', last_name_col='Last name', email_col='email', phone_col='Phone'):
    """
    Identify phone numbers that are NOT exactly 10 digits after cleaning.
    Logs invalid entries and provides summary.
    
    Parameters:
        df1 (pd.DataFrame): DataFrame to analyze
        logger: Logger instance
        first_name_col, last_name_col, email_col, phone_col (str): Column names
    
    Returns:
        pd.DataFrame: DataFrame containing invalid phone number entries
    """
    logger.info("üîç Starting phone number validation")
    
    # Apply cleaning function once
    df1['CleanPhone'] = df1[phone_col].apply(clean_phone_number)

    # Count digits in cleaned phone number
    df1['DigitCount'] = df1['CleanPhone'].apply(lambda x: len(re.sub(r'\D', '', str(x))))

    # Flag entries that are NOT exactly 10 digits
    df1['BadLength'] = df1['DigitCount'] != 10

    # Get entries with incorrect digit count
    bad_length_df = df1[df1['BadLength']]
    
    # Log each invalid phone number
    for idx in bad_length_df.index:
        row = bad_length_df.loc[idx]
        logger.warning(f"INVALID_PHONE - Original: '{row[phone_col]}' | Clean: '{row['CleanPhone']}' | "
                      f"Digits: {row['DigitCount']} | Name: {row.get(first_name_col, 'N/A')} {row.get(last_name_col, 'N/A')} | "
                      f"Email: {row.get(email_col, 'N/A')}")

    # Summary logging
    if not bad_length_df.empty:
        logger.warning(f"üö´ Found {len(bad_length_df)} phone numbers with incorrect length (not 10 digits)")
    else:
        logger.info("‚úÖ All phone numbers have exactly 10 digits after cleaning")
    
    return bad_length_df

def flag_invalid_states(df, logger, state_col='State'):
    """
    Flag and normalize invalid state abbreviations.
    Logs corrections and provides summary.
    
    Parameters:
        df (pd.DataFrame): DataFrame to process
        logger: Logger instance
        state_col (str): Name of the state column
    
    Returns:
        pd.DataFrame: DataFrame containing invalid state entries (before correction)
    """
    logger.info(f"üîç Starting state validation for column '{state_col}'")
    
    # List of official U.S. state abbreviations (uppercase)
    valid_states = {
        'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA',
        'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD',
        'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ',
        'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC',
        'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY'
    }

    # Create normalized version for comparison without modifying original
    df['NormalizedState'] = df[state_col].astype(str).str.strip().str.upper()
    
    # Filter rows where state is not blank and not in valid list
    invalid_states_df = df[
        (df['NormalizedState'] != '') & (~df['NormalizedState'].isin(valid_states))
    ]

    # Log invalid state entries
    for idx in invalid_states_df.index:
        row = invalid_states_df.loc[idx]
        logger.warning(f"INVALID_STATE - Original: '{row[state_col]}' | "
                      f"Name: {row.get('First name', 'N/A')} {row.get('Last name', 'N/A')} | "
                      f"Email: {row.get('email', 'N/A')} | Phone: {row.get('Phone', 'N/A')}")

    # Log and apply normalization corrections
    normalization_count = 0
    for idx in df.index:
        if df.loc[idx, state_col] != df.loc[idx, 'NormalizedState']:
            row = df.loc[idx]
            old_value = row[state_col]
            new_value = row['NormalizedState']
            
            log_correction(logger, "STATE_NORMALIZATION", row, old_value, new_value, state_col)
            df.loc[idx, state_col] = new_value
            normalization_count += 1

    # Clean up temporary column
    df.drop(['NormalizedState'], axis=1, inplace=True)
    
    # Summary logging
    if not invalid_states_df.empty:
        logger.warning(f"üèõÔ∏è Found {len(invalid_states_df)} rows with invalid state abbreviations")
    else:
        logger.info("‚úÖ All state entries are valid")
        
    if normalization_count > 0:
        logger.info(f"üßΩ Normalized {normalization_count} state entries (whitespace/case fixes)")
    
    return invalid_states_df

def process_phone_formatting(df1, logger):
    """
    Apply phone number cleaning and formatting to the DataFrame.
    Logs individual changes and provides summary.
    
    Parameters:
        df1 (pd.DataFrame): DataFrame to process
        logger: Logger instance
    
    Returns:
        dict: Summary statistics of phone processing
    """
    if 'Phone' not in df1.columns:
        logger.warning("üìû Phone column not found - skipping phone formatting")
        return {'changed_count': 0, 'valid_count': 0, 'invalid_count': 0}
    
    logger.info("üìû Starting phone number cleaning and formatting")

    df1['CleanPhone'] = df1['Phone'].apply(clean_phone_number)
    df1['DigitCount'] = df1['CleanPhone'].apply(lambda x: len(re.sub(r'\D', '', str(x))))
    df1['IsValidPhone'] = df1['DigitCount'] == 10

    df1['FormattedPhone'] = df1.apply(
        lambda row: format_phone_number(row['CleanPhone']) if row['IsValidPhone'] else row['Phone'],
        axis=1
    )

    phone_changes = df1['Phone'] != df1['FormattedPhone']
    
    # Log individual phone number changes
    for idx in df1[phone_changes].index:
        row = df1.loc[idx]
        log_correction(logger, "PHONE_FORMAT", row, row['Phone'], row['FormattedPhone'], 'Phone')

    # Apply changes
    df1.loc[phone_changes, 'Phone'] = df1.loc[phone_changes, 'FormattedPhone']

    # Calculate statistics
    changed_count = phone_changes.sum()
    valid_count = df1['IsValidPhone'].sum()
    invalid_count = len(df1) - valid_count

    # Clean up temporary columns
    df1.drop(['CleanPhone', 'DigitCount', 'IsValidPhone', 'FormattedPhone'], axis=1, inplace=True)

    # Summary logging
    logger.info(f"üìä Phone processing summary:")
    logger.info(f"   - {changed_count} phone numbers changed to 999-999-9999 format")
    logger.info(f"   - {valid_count - changed_count} phone numbers already in correct format")
    logger.info(f"   - {invalid_count} phone numbers left unchanged (invalid length)")
    logger.info(f"‚úÖ Phone processing completed")

    return {
        'changed_count': changed_count,
        'valid_count': valid_count,
        'invalid_count': invalid_count
    }

def validate_record_count(df_before, df_after, logger, stage_label=""):
    """
    Validates that the number of records in the DataFrame has not changed during processing.

    Parameters:
        df_before (pd.DataFrame): Original DataFrame before modification.
        df_after (pd.DataFrame): Modified DataFrame after processing.
        logger: Logger instance
        stage_label (str): Optional label to identify the processing stage in logs.

    Returns:
        bool: True if record counts match, False otherwise.
    """
    original_count = len(df_before)
    modified_count = len(df_after)

    logger.info(f"üîç Record Count Validation ({stage_label}):")
    logger.info(f"   üìÑ Original record count: {original_count}")
    logger.info(f"   üìÑ Modified record count: {modified_count}")

    if original_count != modified_count:
        logger.error(f"üö´ Record count mismatch detected during '{stage_label}' stage.")
        logger.error(f"üí° Difference: {original_count - modified_count} records lost.")
        return False
    else:
        logger.info(f"‚úÖ Record count validated: No data loss during '{stage_label}' stage.")
        return True

def standardize_street_types(address_value):
    """
    Standardize street types and directional indicators in address strings.
    
    Parameters:
        address_value (str): Address string to standardize
    
    Returns:
        str: Address with standardized abbreviations
    """
    if pd.isna(address_value) or address_value == '':
        return address_value
    
    address = str(address_value).strip()
    
    # Dictionary of street type standardizations (case insensitive)
    street_types = {
        r'\bStreet\b': 'St',
        r'\bAvenue\b': 'Ave',
        r'\bBoulevard\b': 'Blvd',
        r'\bDrive\b': 'Dr',
        r'\bLane\b': 'Ln',
        r'\bRoad\b': 'Rd',
        r'\bCircle\b': 'Cir',
        r'\bCourt\b': 'Ct',
        r'\bPlace\b': 'Pl',
        r'\bTrail\b': 'Trl',
        r'\bParkway\b': 'Pkwy',
        r'\bHighway\b': 'Hwy',
        r'\bWay\b': 'Way',
        r'\bSquare\b': 'Sq',
        r'\bTerrace\b': 'Ter',
        r'\bAlley\b': 'Aly',
        r'\bCounty Road\b': 'CR',
        r'\bCounty road\b': 'CR',
        r'\bCounty Rd\b': 'CR',
        r'\bC.R.\b': 'CR',
        r'\bState Route\b': 'SR',
        r'\bState Highway\b': 'SH',
        r'\bFarm Road\b': 'FM',
        r'\bRanch Road\b': 'RR',
#        r'\bPark\b': 'Pk',
        r'\bGarden\b': 'Gdn',
        r'\bGardens\b': 'Gdns',
        r'\bCrescent\b': 'Cres',
#        r'\bRidge\b': 'Rdg',
        r'\bHeights\b': 'Hts',
#        r'\bMountain\b': 'Mtn',
        r'\bCreek\b': 'Crk'
#        r'\bValley\b': 'Vly',
#        r'\bView\b': 'Vw'
    }
    
    # Dictionary of directional standardizations
    directional_types = {
        r'\bNorth\b': 'N',
        r'\bSouth\b': 'S',
        r'\bEast\b': 'E',
        r'\bWest\b': 'W',
        r'\bNortheast\b': 'NE',
        r'\bNorthwest\b': 'NW',
        r'\bSoutheast\b': 'SE',
        r'\bSouthwest\b': 'SW'
    }
    
    # Apply street type standardizations
    for pattern, replacement in street_types.items():
        address = re.sub(pattern, replacement, address, flags=re.IGNORECASE)
    
    # Apply directional standardizations
    for pattern, replacement in directional_types.items():
        address = re.sub(pattern, replacement, address, flags=re.IGNORECASE)
    
    return address.strip()

def standardize_unit_types(address_value):
    """
    Standardize unit types in addresses (Apartment, Suite, Unit, etc.).
    
    Parameters:
        address_value (str): Address string to standardize
    
    Returns:
        str: Address with standardized unit types
    """
    if pd.isna(address_value) or address_value == '':
        return address_value
    
    address = str(address_value).strip()
    
    # Dictionary of unit type standardizations
    unit_types = {
        r'\bApartment\b': 'Apt',
        r'\bSuite\b': 'Ste',
        r'\bUnit\b': 'Unit',
        r'\bBuilding\b': 'Bldg',
        r'\bFloor\b': 'Fl',
        r'\bRoom\b': 'Rm',
        r'\bOffice\b': 'Ofc',
        r'\bDepartment\b': 'Dept',
        r'\bTrailer\b': 'Trlr',
        r'\bSpace\b': 'Spc',
        r'\bLot\b': 'Lot'
    }
    
    # Apply unit type standardizations
    for pattern, replacement in unit_types.items():
        address = re.sub(pattern, replacement, address, flags=re.IGNORECASE)
    
    return address.strip()

def format_address_standardization(df, logger, address_col='Address'):
    """
    Apply address standardization to the specified address column.
    Logs individual changes and provides summary.
    
    Parameters:
        df (pd.DataFrame): DataFrame to process
        logger: Logger instance
        address_col (str): Name of the address column to standardize
    
    Returns:
        dict: Summary statistics of address standardization
    """
    if address_col not in df.columns:
        logger.warning(f"üè† Address column '{address_col}' not found - skipping address standardization")
        return {'street_changes': 0, 'unit_changes': 0, 'total_processed': 0}
    
    logger.info(f"üè† Starting address standardization for column '{address_col}'")
    
    street_changes = 0
    unit_changes = 0
    total_processed = 0
    
    # Create working copies to track changes
    df['StandardizedStreet'] = df[address_col].apply(standardize_street_types)
    df['StandardizedUnit'] = df['StandardizedStreet'].apply(standardize_unit_types)
    
    # Process each row and log changes
    for idx in df.index:
        row = df.loc[idx]
        original_address = str(row[address_col]) if not pd.isna(row[address_col]) else ''
        street_standardized = str(row['StandardizedStreet'])
        final_standardized = str(row['StandardizedUnit'])
        
        # Skip empty addresses
        if original_address.strip() == '':
            continue
            
        total_processed += 1
        
        # Log street type changes
        if original_address != street_standardized:
            log_correction(logger, "ADDRESS_STREET_TYPE", row, original_address, street_standardized, address_col)
            street_changes += 1
        
        # Log unit type changes (compare final to street-only standardized)
        if street_standardized != final_standardized:
            log_correction(logger, "ADDRESS_UNIT_TYPE", row, street_standardized, final_standardized, address_col)
            unit_changes += 1
        
        # Apply the final standardized address
        df.loc[idx, address_col] = final_standardized
    
    # Clean up temporary columns
    df.drop(['StandardizedStreet', 'StandardizedUnit'], axis=1, inplace=True)
    
    # Summary logging
    logger.info(f"üìä Address standardization summary for '{address_col}':")
    logger.info(f"   - {street_changes} street type standardizations")
    logger.info(f"   - {unit_changes} unit type standardizations")
    logger.info(f"   - {total_processed} addresses processed")
    logger.info(f"‚úÖ Address standardization completed for '{address_col}'")
    
    return {
        'street_changes': street_changes,
        'unit_changes': unit_changes,
        'total_processed': total_processed
    }

def clean_address_spacing_formatting(df, logger, address_col='Address'):
    """
    Clean up spacing and basic formatting issues in addresses.
    Removes extra spaces, standardizes spacing around punctuation.
    
    Parameters:
        df (pd.DataFrame): DataFrame to process
        logger: Logger instance
        address_col (str): Name of the address column to clean
    
    Returns:
        dict: Summary statistics of address cleaning
    """
    if address_col not in df.columns:
        logger.warning(f"üè† Address column '{address_col}' not found - skipping address spacing cleanup")
        return {'spacing_changes': 0, 'total_processed': 0}
    
    logger.info(f"üè† Starting address spacing cleanup for column '{address_col}'")
    
    spacing_changes = 0
    total_processed = 0
    
    for idx in df.index:
        row = df.loc[idx]
        original_address = str(row[address_col]) if not pd.isna(row[address_col]) else ''
        
        if original_address.strip() == '':
            continue
            
        total_processed += 1
        
        # Apply spacing cleanup
        cleaned_address = original_address.strip()
        
        # Remove multiple spaces
        cleaned_address = re.sub(r'\s+', ' ', cleaned_address)
        
        # Standardize spacing around commas
        cleaned_address = re.sub(r'\s*,\s*', ', ', cleaned_address)
        
        # Standardize spacing around periods, but preserve P.O. Box format
        # First, temporarily replace P.O. Box patterns to protect them
        cleaned_address = re.sub(r'\bP\.O\.\s*Box\b', 'TEMP_PO_BOX', cleaned_address, flags=re.IGNORECASE)
        cleaned_address = re.sub(r'\s*\.\s*', '. ', cleaned_address)
        # Restore P.O. Box format
        cleaned_address = re.sub(r'\bTEMP_PO_BOX\b', 'P.O. Box', cleaned_address)
        
        # Remove trailing comma or period followed by space
        cleaned_address = re.sub(r'[,.]$', '', cleaned_address).strip()
        
        # Log changes
        if original_address != cleaned_address:
            log_correction(logger, "ADDRESS_SPACING", row, original_address, cleaned_address, address_col)
            df.loc[idx, address_col] = cleaned_address
            spacing_changes += 1
    
    # Summary logging
    logger.info(f"üìä Address spacing cleanup summary for '{address_col}':")
    logger.info(f"   - {spacing_changes} spacing corrections")
    logger.info(f"   - {total_processed} addresses processed")
    logger.info(f"‚úÖ Address spacing cleanup completed for '{address_col}'")
    
    return {
        'spacing_changes': spacing_changes,
        'total_processed': total_processed
    }

def get_latest_cleaned_file(output_dir, base_name):
    """
    Finds the most recent cleaned Excel file from a previous run based on naming convention.

    Parameters:
        output_dir (str): Directory where cleaned files are saved.
        base_name (str): Base name prefix used in cleaned file naming.

    Returns:
        str or None: Full path to the most recent cleaned file, or None if none found.
    """
    pattern = os.path.join(output_dir, f"{base_name}_*.xlsx")
    matching_files = glob.glob(pattern)

    if not matching_files:
        return None

    # Sort by modified time, descending
    matching_files.sort(key=os.path.getmtime, reverse=True)
    return matching_files[0]

#MAIN#############################################################

if __name__ == "__main__":
    # Setup logging
    logger = setup_logging(log_filepath)
    logger.info(f"üöÄ Starting DBG data cleaning process")

    script_name = os.path.basename(__file__)
    logger.info(f"Running script: {script_name}")
    logger.info(f"Log file: {log_filepath}")

    # üîç Check for most recent cleaned file from previous run
    latest_cleaned_file = get_latest_cleaned_file(output_dir2, base_name2)

    if latest_cleaned_file:
        logger.info("üîÑ Previous cleaned file found")
        logger.info(f"   Last cleaned file: {latest_cleaned_file}")
        print("\nüîÑ Do you want to use the last cleaned file as the new input?")
        print(f"   Last cleaned file found:\n{latest_cleaned_file}")
        user_choice = input("   Type 'Y' to substitute original input file with the last cleaned file, or press Enter to continue with the original: ").strip().lower()

        if user_choice == 'y':
            file_path1 = latest_cleaned_file
            logger.info(f"üìÅ Input file substituted with: {file_path1}")
        else:
            logger.info(f"üìÅ Proceeding with original input file: {file_path1}")
    else:
        logger.info("‚ö†Ô∏è No previously cleaned file found. Proceeding with original input file.")

    file_path2 = full_path2

    # üßæ Load input Excel file
    try:
        df1 = pd.read_excel(file_path1, engine='openpyxl')
        logger.info(f'‚úÖ Input File Loaded: {file_path1}')
        logger.info(f'üìä Input DataFrame shape: {df1.shape}')
    except Exception as e:
        logger.error(f"‚ùå Failed to load input file: {e}")
        exit(1)

    # Store original DataFrame for comparison BEFORE any modifications
    df_original = df1.copy()

    # Process invalid states
    invalid_states_df = flag_invalid_states(df1, logger)

    # Process invalid phone numbers
    invalid_phone_df = get_invalid_phone_number(df1, logger)

    # Clean contact fields (email and Phone spaces)
    df1 = clean_contact_fields_with_logging(df1, logger)

    # Process address spacing cleanup
    address_spacing_stats = clean_address_spacing_formatting(df1, logger, 'Address')

    # Process address standardization
    address_standard_stats = format_address_standardization(df1, logger, 'Address')

    # Process phone number formatting
    phone_stats = process_phone_formatting(df1, logger)

    # Drop any remaining temporary calculation columns
    temp_cols = ['CleanPhone', 'DigitCount', 'BadLength']
    for col in temp_cols:
        if col in df1.columns:
            df1.drop([col], axis=1, inplace=True)

    # Check to see if there were changes between the original and potentially modified data
    data_changed = not df1.equals(df_original)
    logger.info(f"üìã Data modification check: {'Changes detected' if data_changed else 'No changes detected'}")

    # Validation check for before and after record count
    record_count_valid = validate_record_count(df_original, df1, logger, "data cleaning")

    if not record_count_valid:
        logger.error("üö´ STOPPING: Record count validation failed. No output file will be created.")
        logger.error("üí° Please review the cleaning process - data may have been accidentally deleted.")
    elif data_changed:
        logger.info("üìù Data has been modified and validation passed. Writing cleaned data to output file...")
        try:
            df1.to_excel(full_path2, index=False)
            logger.info(f"‚úÖ Cleaned data successfully written to: {full_path2}")
        except Exception as e:
            logger.error(f"‚ùå Error writing to file: {e}")
    else:
        logger.info("‚úÖ No changes detected in data. Skipping output file creation.")
        logger.info("üí° Original file is already clean - no output file needed.")

    # Log final summary statistics
    logger.info("üìä Final Processing Summary:")
    logger.info(f"   - Invalid states found: {len(invalid_states_df)}")
    logger.info(f"   - Invalid phone numbers found: {len(invalid_phone_df)}")
    logger.info(f"   - Phone formatting changes: {phone_stats.get('changed_count', 0)}")
    logger.info(f"   - Address spacing corrections: {address_spacing_stats.get('spacing_changes', 0)}")
    logger.info(f"   - Address street standardizations: {address_standard_stats.get('street_changes', 0)}")
    logger.info(f"   - Address unit standardizations: {address_standard_stats.get('unit_changes', 0)}")

    logger.info("üèÅ DBG data cleaning process completed")
    logger.info(f"üìÑ Detailed log saved to: {log_filepath}")